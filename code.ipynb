{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68967d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting optimized full load...\n",
      "Computing aggregations on full dataset...\n",
      "DBSCAN (full) found 103 clusters\n",
      "\n",
      "✅ All done. Outputs saved to: outputs\n",
      "Check these core files in the outputs folder:\n",
      " - accident_heatmap.html\n",
      " - accidents_by_hour.csv\n",
      " - accidents_by_hour.png\n",
      " - accidents_by_roadflags.csv\n",
      " - accidents_by_roadflags.png\n",
      " - accidents_by_weather.png\n",
      " - accidents_by_weathergroup.csv\n",
      " - accidents_by_weekday.csv\n",
      " - accidents_by_weekday.png\n",
      " - hourly_by_weather.csv\n",
      " - map_sample.csv\n",
      " - sample_inspect.csv\n",
      " - severity_by_weather.csv\n",
      " - top_clusters.html\n"
     ]
    }
   ],
   "source": [
    "# analyze_accidents_updated.py\n",
    "\"\"\"\n",
    "Analyze US Accidents CSV to identify patterns related to:\n",
    "- road conditions\n",
    "- weather\n",
    "- time of day\n",
    "\n",
    "Outputs:\n",
    " - PNG charts in ./outputs/\n",
    " - CSV summaries in ./outputs/\n",
    " - HTML maps: accident_heatmap.html and top_clusters.html in ./outputs/\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "# --------- CONFIG ----------\n",
    "DATA_PATH = r\"M:\\Internship task\\Task 5\\US_Accidents_March23.csv\"\n",
    "OUT_DIR = \"outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# columns we expect (common names in the Kaggle dataset)\n",
    "LAT_COL = \"Start_Lat\"\n",
    "LNG_COL = \"Start_Lng\"\n",
    "TIME_COL = \"Start_Time\"\n",
    "WEATHER_COL = \"Weather_Condition\"\n",
    "SEV_COL = \"Severity\"\n",
    "\n",
    "# candidate road condition boolean fields (if present)\n",
    "ROAD_COLS = [\"Traffic_Signal\", \"Crossing\", \"Junction\", \"Station\", \"Stop\", \"Turning_Loop\",\n",
    "             \"Amenity\", \"Bump\", \"Give_Way\", \"No_Exit\", \"Railway\", \"Roundabout\", \"Traffic_Calming\"]\n",
    "\n",
    "# ---------- HELPERS ----------\n",
    "def try_load_with_dtypes(path):\n",
    "    \"\"\"Attempt to load full CSV with memory-optimized dtypes.\"\"\"\n",
    "    dtype_map = {\n",
    "        \"Severity\": \"int8\",\n",
    "        \"State\": \"category\",\n",
    "        \"Zipcode\": \"category\",\n",
    "        \"Weather_Condition\": \"category\",\n",
    "        \"Country\": \"category\",\n",
    "        \"Timezone\": \"category\",\n",
    "        \"Side\": \"category\"\n",
    "    }\n",
    "    use_parse_dates = [TIME_COL]\n",
    "    print(\"Attempting optimized full load...\")\n",
    "    df = pd.read_csv(path, dtype=dtype_map, parse_dates=use_parse_dates, low_memory=True)\n",
    "    return df\n",
    "\n",
    "def load_chunk_aggregations(path, chunk_size=250_000):\n",
    "    \"\"\"\n",
    "    Read CSV in chunks to compute aggregated summaries without loading full file.\n",
    "    Returns aggregated DataFrames and a sampled dataframe for mapping.\n",
    "    \"\"\"\n",
    "    print(\"Falling back to chunked processing (safe for low memory).\")\n",
    "    chunker = pd.read_csv(path, parse_dates=[TIME_COL], chunksize=chunk_size, low_memory=True)\n",
    "\n",
    "    # accumulators\n",
    "    hour_series = pd.Series(0, index=range(24), dtype=\"int64\")\n",
    "    weekday_series = pd.Series(0, index=[\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"], dtype=\"int64\")\n",
    "    weather_counts = {}\n",
    "    road_counts = {col: 0 for col in ROAD_COLS}\n",
    "    severity_weather = {}\n",
    "    sample_rows = []\n",
    "\n",
    "    for i, chunk in enumerate(chunker):\n",
    "        # basic cleaning\n",
    "        chunk = chunk.dropna(subset=[TIME_COL])\n",
    "        chunk[TIME_COL] = pd.to_datetime(chunk[TIME_COL], errors=\"coerce\")\n",
    "        chunk = chunk.dropna(subset=[TIME_COL])\n",
    "        chunk[\"hour\"] = chunk[TIME_COL].dt.hour\n",
    "        chunk[\"weekday\"] = chunk[TIME_COL].dt.day_name()\n",
    "\n",
    "        # hours\n",
    "        hour_series = hour_series.add(chunk[\"hour\"].value_counts().reindex(range(24), fill_value=0), fill_value=0)\n",
    "        # weekdays\n",
    "        weekday_series = weekday_series.add(chunk[\"weekday\"].value_counts().reindex(weekday_series.index, fill_value=0), fill_value=0)\n",
    "\n",
    "        # weather counts\n",
    "        if WEATHER_COL in chunk.columns:\n",
    "            wc_counts = chunk[WEATHER_COL].fillna(\"Unknown\").value_counts()\n",
    "            for k,v in wc_counts.items():\n",
    "                weather_counts[k] = weather_counts.get(k, 0) + int(v)\n",
    "\n",
    "        # road flags\n",
    "        for col in ROAD_COLS:\n",
    "            if col in chunk.columns:\n",
    "                # sum True values (some columns are boolean or 0/1)\n",
    "                road_counts[col] += int(chunk[col].fillna(False).astype(bool).sum())\n",
    "\n",
    "        # severity by weather\n",
    "        if (WEATHER_COL in chunk.columns) and (SEV_COL in chunk.columns):\n",
    "            pivot = chunk.groupby([WEATHER_COL, SEV_COL]).size()\n",
    "            for (w,s), count in pivot.items():\n",
    "                severity_weather.setdefault(w, {}).setdefault(s, 0)\n",
    "                severity_weather[w][s] += int(count)\n",
    "\n",
    "        # sample rows for mapping (reservoir sampling up to many points)\n",
    "        if LAT_COL in chunk.columns and LNG_COL in chunk.columns:\n",
    "            coords = chunk[[LAT_COL, LNG_COL]].dropna()\n",
    "            n_take = min(len(coords), 5000)  # take up to 5k per chunk\n",
    "            if n_take > 0:\n",
    "                sample_rows.append(coords.sample(n=n_take, random_state=42))\n",
    "\n",
    "        print(f\"Processed chunk {i+1}\", end=\"\\r\")\n",
    "\n",
    "    # combine sample rows\n",
    "    sample_df = pd.concat(sample_rows, ignore_index=True) if sample_rows else pd.DataFrame(columns=[LAT_COL, LNG_COL])\n",
    "    # convert dicts -> pandas objects\n",
    "    weather_s = pd.Series(weather_counts).sort_values(ascending=False)\n",
    "    road_s = pd.Series(road_counts).sort_values(ascending=False)\n",
    "    # severity by weather -> DataFrame\n",
    "    sev_weather_df = pd.DataFrame.from_dict(severity_weather, orient=\"index\").fillna(0).sort_values(0, ascending=False)\n",
    "\n",
    "    return hour_series.astype(int), weekday_series.astype(int), weather_s, road_s, sev_weather_df, sample_df\n",
    "\n",
    "def group_weather_label(s):\n",
    "    \"\"\"Map raw weather_text -> grouped simpler labels\"\"\"\n",
    "    s = str(s).lower()\n",
    "    if any(x in s for x in [\"rain\",\"drizzle\",\"shower\",\"spray\"]):\n",
    "        return \"Rain\"\n",
    "    if any(x in s for x in [\"snow\",\"sleet\",\"blizzard\",\"flurr\", \"ice\"]):\n",
    "        return \"Snow/Ice\"\n",
    "    if any(x in s for x in [\"fog\",\"mist\",\"haze\",\"smoke\",\"squall\"]):\n",
    "        return \"Low Visibility\"\n",
    "    if any(x in s for x in [\"clear\",\"sun\",\"fair\"]):\n",
    "        return \"Clear\"\n",
    "    if any(x in s for x in [\"cloud\",\"overcast\"]):\n",
    "        return \"Cloudy\"\n",
    "    if any(x in s for x in [\"thunder\",\"storm\",\"lightning\"]):\n",
    "        return \"Thunderstorm\"\n",
    "    return \"Other\"\n",
    "\n",
    "# ---------- MAIN ----------\n",
    "def main():\n",
    "    # try full optimized load first\n",
    "    try:\n",
    "        df = try_load_with_dtypes(DATA_PATH)\n",
    "        full_load = True\n",
    "    except MemoryError:\n",
    "        df = None\n",
    "        full_load = False\n",
    "\n",
    "    if full_load:\n",
    "        # ensure time column exists and parse\n",
    "        if TIME_COL not in df.columns:\n",
    "            raise ValueError(f\"{TIME_COL} not found in file.\")\n",
    "        df = df.dropna(subset=[TIME_COL])\n",
    "        df[TIME_COL] = pd.to_datetime(df[TIME_COL], errors=\"coerce\")\n",
    "        df = df.dropna(subset=[TIME_COL])\n",
    "\n",
    "        # time features\n",
    "        df[\"hour\"] = df[TIME_COL].dt.hour\n",
    "        df[\"weekday\"] = df[TIME_COL].dt.day_name()\n",
    "        df[\"month\"] = df[TIME_COL].dt.month_name()\n",
    "\n",
    "        # group weather to simpler categories\n",
    "        if WEATHER_COL in df.columns:\n",
    "            df[\"weather_group\"] = df[WEATHER_COL].apply(group_weather_label)\n",
    "        else:\n",
    "            df[\"weather_group\"] = \"Unknown\"\n",
    "\n",
    "        # Road booleans - normalize\n",
    "        present_road_cols = [c for c in ROAD_COLS if c in df.columns]\n",
    "        for c in present_road_cols:\n",
    "            df[c] = df[c].fillna(False).astype(bool)\n",
    "\n",
    "        # --- Aggregations & charts ---\n",
    "        print(\"Computing aggregations on full dataset...\")\n",
    "        accidents_by_hour = df[\"hour\"].value_counts().reindex(range(24), fill_value=0).sort_index()\n",
    "        accidents_by_weekday = df[\"weekday\"].value_counts().reindex(\n",
    "            [\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"], fill_value=0\n",
    "        )\n",
    "        weather_counts = df[\"weather_group\"].value_counts()\n",
    "        road_counts = {c: int(df[c].sum()) for c in present_road_cols}\n",
    "\n",
    "        # severity by weather\n",
    "        if SEV_COL in df.columns:\n",
    "            severity_by_weather = df.groupby([\"weather_group\", SEV_COL]).size().unstack(fill_value=0)\n",
    "            severity_by_weather.to_csv(os.path.join(OUT_DIR, \"severity_by_weather.csv\"))\n",
    "\n",
    "        # save aggregates as CSV\n",
    "        accidents_by_hour.to_csv(os.path.join(OUT_DIR, \"accidents_by_hour.csv\"), header=[\"count\"])\n",
    "        accidents_by_weekday.to_csv(os.path.join(OUT_DIR, \"accidents_by_weekday.csv\"), header=[\"count\"])\n",
    "        weather_counts.to_csv(os.path.join(OUT_DIR, \"accidents_by_weathergroup.csv\"), header=[\"count\"])\n",
    "        pd.Series(road_counts).to_csv(os.path.join(OUT_DIR, \"accidents_by_roadflags.csv\"), header=[\"count\"])\n",
    "\n",
    "        # --- PLOTS ---\n",
    "        plt.figure(figsize=(10,4))\n",
    "        plt.plot(accidents_by_hour.index, accidents_by_hour.values, marker=\"o\")\n",
    "        plt.title(\"Accidents by Hour\")\n",
    "        plt.xlabel(\"Hour (0-23)\")\n",
    "        plt.ylabel(\"Number of Accidents\")\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUT_DIR, \"accidents_by_hour.png\"), dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure(figsize=(9,4))\n",
    "        accidents_by_weekday.plot(kind=\"bar\")\n",
    "        plt.title(\"Accidents by Weekday\")\n",
    "        plt.ylabel(\"Number of Accidents\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUT_DIR, \"accidents_by_weekday.png\"), dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure(figsize=(9,4))\n",
    "        weather_counts.head(12).plot(kind=\"bar\")\n",
    "        plt.title(\"Top Weather Groups\")\n",
    "        plt.ylabel(\"Number of Accidents\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUT_DIR, \"accidents_by_weather.png\"), dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "        if road_counts:\n",
    "            plt.figure(figsize=(10,4))\n",
    "            pd.Series(road_counts).sort_values(ascending=False).plot(kind=\"bar\")\n",
    "            plt.title(\"Accidents near road-condition flags (counts)\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(OUT_DIR, \"accidents_by_roadflags.png\"), dpi=200)\n",
    "            plt.close()\n",
    "\n",
    "        # --- Mapping: sample for HeatMap & clustering ---\n",
    "        if LAT_COL in df.columns and LNG_COL in df.columns:\n",
    "            coords = df[[LAT_COL, LNG_COL]].dropna()\n",
    "            sample_n = min(200_000, len(coords))\n",
    "            heat_sample = coords.sample(n=sample_n, random_state=42)\n",
    "            center_lat = float(heat_sample[LAT_COL].median())\n",
    "            center_lng = float(heat_sample[LNG_COL].median())\n",
    "\n",
    "            # save heatmap\n",
    "            m = folium.Map(location=[center_lat, center_lng], zoom_start=6, tiles=\"CartoDB positron\")\n",
    "            HeatMap(heat_sample.values.tolist(), radius=7, blur=10, min_opacity=0.3).add_to(m)\n",
    "            m.save(os.path.join(OUT_DIR, \"accident_heatmap.html\"))\n",
    "\n",
    "            # DBSCAN clusters on a smaller sample\n",
    "            cluster_sample = heat_sample.sample(n=min(50_000, len(heat_sample)), random_state=42)\n",
    "            coords_np = cluster_sample.to_numpy()\n",
    "            coords_rad = np.radians(coords_np)\n",
    "            kms_per_radian = 6371.0088\n",
    "\n",
    "            # ---- UPDATED DBSCAN PARAMETERS ----\n",
    "            epsilon = 10.0 / kms_per_radian  # 10 km radius\n",
    "            db = DBSCAN(eps=epsilon, min_samples=30, algorithm=\"ball_tree\", metric=\"haversine\")\n",
    "            labels = db.fit_predict(coords_rad)\n",
    "\n",
    "            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "            print(f\"DBSCAN (full) found {n_clusters} clusters\")\n",
    "\n",
    "            cluster_sample = cluster_sample.reset_index(drop=True).copy()\n",
    "            cluster_sample[\"cluster\"] = labels\n",
    "\n",
    "            # create cluster map\n",
    "            cluster_counts = cluster_sample[cluster_sample.cluster != -1][\"cluster\"].value_counts().head(20)\n",
    "            m2 = folium.Map(location=[center_lat, center_lng], zoom_start=6, tiles=\"CartoDB dark_matter\")\n",
    "            if cluster_counts.empty:\n",
    "                print(\"No clusters found in full-load DBSCAN. Try adjusting eps/min_samples.\")\n",
    "            else:\n",
    "                for cl in cluster_counts.index:\n",
    "                    member = cluster_sample[cluster_sample.cluster == cl]\n",
    "                    centroid = [float(member[LAT_COL].mean()), float(member[LNG_COL].mean())]\n",
    "                    folium.CircleMarker(location=centroid,\n",
    "                                        radius=8 + math.log(len(member) + 1),\n",
    "                                        color=\"red\",\n",
    "                                        fill=True,\n",
    "                                        fill_opacity=0.7,\n",
    "                                        popup=f\"Cluster {int(cl)} ({len(member)} accidents)\").add_to(m2)\n",
    "            m2.save(os.path.join(OUT_DIR, \"top_clusters.html\"))\n",
    "\n",
    "            # save small sample CSV for manual inspection\n",
    "            heat_sample.sample(n=min(10000, len(heat_sample))).to_csv(os.path.join(OUT_DIR, \"map_sample.csv\"), index=False)\n",
    "\n",
    "    else:\n",
    "        # chunked approach - compute aggregates and sample for mapping\n",
    "        (accidents_by_hour, accidents_by_weekday,\n",
    "         weather_counts, road_counts, severity_by_weather,\n",
    "         sample_df) = load_chunk_aggregations(DATA_PATH)\n",
    "\n",
    "        # save CSVs\n",
    "        accidents_by_hour.to_csv(os.path.join(OUT_DIR, \"accidents_by_hour.csv\"), header=[\"count\"])\n",
    "        accidents_by_weekday.to_csv(os.path.join(OUT_DIR, \"accidents_by_weekday.csv\"), header=[\"count\"])\n",
    "        weather_counts.to_csv(os.path.join(OUT_DIR, \"accidents_by_weather_raw.csv\"), header=[\"count\"])\n",
    "        road_counts.to_csv(os.path.join(OUT_DIR, \"accidents_by_roadflags_raw.csv\"), header=[\"count\"])\n",
    "        severity_by_weather.to_csv(os.path.join(OUT_DIR, \"severity_by_weather_raw.csv\"))\n",
    "\n",
    "        # PLOTS (same appearance as full-load version)\n",
    "        plt.figure(figsize=(10,4))\n",
    "        plt.plot(accidents_by_hour.index, accidents_by_hour.values, marker=\"o\")\n",
    "        plt.title(\"Accidents by Hour (chunked)\")\n",
    "        plt.xlabel(\"Hour (0-23)\")\n",
    "        plt.ylabel(\"Number of Accidents\")\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUT_DIR, \"accidents_by_hour_chunked.png\"), dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure(figsize=(9,4))\n",
    "        accidents_by_weekday.plot(kind=\"bar\")\n",
    "        plt.title(\"Accidents by Weekday (chunked)\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUT_DIR, \"accidents_by_weekday_chunked.png\"), dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure(figsize=(9,4))\n",
    "        if not weather_counts.empty:\n",
    "            weather_counts.head(12).plot(kind=\"bar\")\n",
    "            plt.title(\"Top Weather Conditions (chunked)\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(OUT_DIR, \"accidents_by_weather_chunked.png\"), dpi=200)\n",
    "            plt.close()\n",
    "\n",
    "        # mapping from sample_df\n",
    "        if not sample_df.empty:\n",
    "            sample_df = sample_df.dropna()\n",
    "            sample_n = min(100_000, len(sample_df))\n",
    "            heat_sample = sample_df.sample(n=sample_n, random_state=42)\n",
    "            center_lat = float(heat_sample[LAT_COL].median())\n",
    "            center_lng = float(heat_sample[LNG_COL].median())\n",
    "\n",
    "            m = folium.Map(location=[center_lat, center_lng], zoom_start=6, tiles=\"CartoDB positron\")\n",
    "            HeatMap(heat_sample.values.tolist(), radius=7, blur=10, min_opacity=0.3).add_to(m)\n",
    "            m.save(os.path.join(OUT_DIR, \"accident_heatmap_chunked.html\"))\n",
    "\n",
    "            # DBSCAN on smaller sample\n",
    "            cluster_sample = heat_sample.sample(n=min(30_000, len(heat_sample)), random_state=42)\n",
    "            coords_np = cluster_sample.to_numpy()\n",
    "            coords_rad = np.radians(coords_np)\n",
    "            kms_per_radian = 6371.0088\n",
    "\n",
    "            # ---- UPDATED DBSCAN PARAMETERS (chunked) ----\n",
    "            epsilon = 10.0 / kms_per_radian  # 10 km radius\n",
    "            db = DBSCAN(eps=epsilon, min_samples=40, algorithm=\"ball_tree\", metric=\"haversine\")\n",
    "            labels = db.fit_predict(coords_rad)\n",
    "\n",
    "            n_clusters_chunked = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "            print(f\"DBSCAN (chunked) found {n_clusters_chunked} clusters\")\n",
    "\n",
    "            cluster_sample = cluster_sample.reset_index(drop=True).copy()\n",
    "            cluster_sample[\"cluster\"] = labels\n",
    "            cluster_counts = cluster_sample[cluster_sample.cluster != -1][\"cluster\"].value_counts().head(20)\n",
    "\n",
    "            m2 = folium.Map(location=[center_lat, center_lng], zoom_start=6, tiles=\"CartoDB dark_matter\")\n",
    "            if cluster_counts.empty:\n",
    "                print(\"No clusters found in chunked DBSCAN. Try adjusting eps/min_samples.\")\n",
    "            else:\n",
    "                for cl in cluster_counts.index:\n",
    "                    member = cluster_sample[cluster_sample.cluster == cl]\n",
    "                    centroid = [float(member[LAT_COL].mean()), float(member[LNG_COL].mean())]\n",
    "                    folium.CircleMarker(location=centroid,\n",
    "                                        radius=8 + math.log(len(member) + 1),\n",
    "                                        color=\"red\",\n",
    "                                        fill=True,\n",
    "                                        fill_opacity=0.7,\n",
    "                                        popup=f\"Cluster {int(cl)} ({len(member)} accidents)\").add_to(m2)\n",
    "            m2.save(os.path.join(OUT_DIR, \"top_clusters_chunked.html\"))\n",
    "\n",
    "            heat_sample.sample(n=min(5000, len(heat_sample))).to_csv(os.path.join(OUT_DIR, \"map_sample_chunked.csv\"), index=False)\n",
    "\n",
    "    print(\"\\n✅ All done. Outputs saved to:\", OUT_DIR)\n",
    "    print(\"Check these core files in the outputs folder:\")\n",
    "    for f in sorted(os.listdir(OUT_DIR)):\n",
    "        print(\" -\", f)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a5e4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chunk_aggregations(path, chunk_size=250_000):\n",
    "    \"\"\"Process CSV in chunks to avoid memory issues and compute summaries.\"\"\"\n",
    "    chunker = pd.read_csv(path, parse_dates=[TIME_COL], chunksize=chunk_size, low_memory=True)\n",
    "\n",
    "    hour_series = pd.Series(0, index=range(24), dtype=\"int64\")\n",
    "    weekday_series = pd.Series(0, index=[\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"], dtype=\"int64\")\n",
    "    weather_counts = {}\n",
    "    road_counts = {col: 0 for col in ROAD_COLS}\n",
    "    sample_rows = []\n",
    "\n",
    "    for chunk in chunker:\n",
    "        chunk = chunk.dropna(subset=[TIME_COL])\n",
    "        chunk[TIME_COL] = pd.to_datetime(chunk[TIME_COL], errors=\"coerce\")\n",
    "        chunk[\"hour\"] = chunk[TIME_COL].dt.hour\n",
    "        chunk[\"weekday\"] = chunk[TIME_COL].dt.day_name()\n",
    "\n",
    "        # update counters\n",
    "        hour_series += chunk[\"hour\"].value_counts().reindex(range(24), fill_value=0)\n",
    "        weekday_series += chunk[\"weekday\"].value_counts().reindex(weekday_series.index, fill_value=0)\n",
    "\n",
    "        # road flags & weather\n",
    "        for col in ROAD_COLS:\n",
    "            if col in chunk.columns:\n",
    "                road_counts[col] += int(chunk[col].fillna(False).astype(bool).sum())\n",
    "\n",
    "        if LAT_COL in chunk.columns and LNG_COL in chunk.columns:\n",
    "            coords = chunk[[LAT_COL, LNG_COL]].dropna()\n",
    "            if not coords.empty:\n",
    "                sample_rows.append(coords.sample(n=min(5000, len(coords)), random_state=42))\n",
    "\n",
    "    sample_df = pd.concat(sample_rows, ignore_index=True) if sample_rows else pd.DataFrame()\n",
    "    return hour_series, weekday_series, pd.Series(weather_counts), pd.Series(road_counts), sample_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75437331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Aggregations ---\n",
    "accidents_by_hour = df[\"hour\"].value_counts().reindex(range(24), fill_value=0).sort_index()\n",
    "accidents_by_weekday = df[\"weekday\"].value_counts().reindex(\n",
    "    [\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"], fill_value=0\n",
    ")\n",
    "weather_counts = df[\"weather_group\"].value_counts()\n",
    "\n",
    "# --- Visualization ---\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(accidents_by_hour.index, accidents_by_hour.values, marker=\"o\")\n",
    "plt.title(\"Accidents by Hour\")\n",
    "plt.xlabel(\"Hour (0-23)\")\n",
    "plt.ylabel(\"Number of Accidents\")\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(OUT_DIR, \"accidents_by_hour.png\"), dpi=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1559e593",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = df[[LAT_COL, LNG_COL]].dropna().sample(n=200_000, random_state=42)\n",
    "m = folium.Map(location=[coords[LAT_COL].median(), coords[LNG_COL].median()],\n",
    "               zoom_start=6, tiles=\"CartoDB positron\")\n",
    "HeatMap(coords.values.tolist(), radius=7, blur=10, min_opacity=0.3).add_to(m)\n",
    "m.save(os.path.join(OUT_DIR, \"accident_heatmap.html\"))\n",
    "\n",
    "# DBSCAN Clustering\n",
    "coords_rad = np.radians(coords.to_numpy())\n",
    "epsilon = 10.0 / 6371.0088  # 10 km\n",
    "db = DBSCAN(eps=epsilon, min_samples=30, metric=\"haversine\").fit(coords_rad)\n",
    "labels = db.labels_\n",
    "print(\"DBSCAN clusters found:\", len(set(labels)) - (1 if -1 in labels else 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0895fe7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
